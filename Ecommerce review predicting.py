# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/172OclLoMBA8z285N19YVg2WjtxHTb6EG

## **Predicting E-zcommerce product recommendation ratings from review**
### **Main Objective:** Leverage the review text attributes to predict the recommendation rating,

**Importing useful libraries**
"""

import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression

"""### Importing the dataset"""

data = pd.read_csv('https://raw.githubusercontent.com/dipanjanS/feature_engineering_session_dhs18/master/ecommerce_product_ratings_prediction/Womens%20Clothing%20E-Commerce%20Reviews.csv', keep_default_na=False)
data.head()

"""## **Data Preprocessing**


*   We will merge all the review columns into one.
*   We will convert the 5 star system into two diffrent category with ratings 0 (rating<3) and 1 (rating>3).


"""

data['Reviews'] = data.Title + ' ' + data['Review Text']
data['Rating'] = [1 if rating > 3 else 0 for rating in data['Rating']]
# for i in data['Rating']:
#   if i > 3:
#     data['Rating'] == 1
#   else:
#     data['Rating'] == 0 
data = data[['Reviews','Rating']]

pd.set_option('display.max_colwidth', 1000)
data.head()

data.Rating.value_counts()

data.info()

# data['Reviews'].dropna()
for i in data['Reviews']:
  if i != '':
    pass
  else:
    data['Reviews'].remove(i)
data.info()

"""### **Splitting the data into dependent and independent variables x , y**"""

x = data.iloc[:,0:1]
y = data.iloc[:,1:2]

"""### **further splitting the data as Train and Test data**"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)

"""Counter produces a dictionary with the letters as keys and their frequency as values, used to count unique values including the alphabets in a word."""

from collections import Counter
Counter(y_train), Counter(y_test)

# we have not used Counter for x because it has a lot of alphabets and it'll further be off no use to us.

"""# Basic NLP with counting features
## We will use certain features which will help us in improving text classification models such as:


*   **Word Count:** total no of word 
*   **Character count:** total no of character
*   **Average world density:** average length of the words used 
*   **Punctuation count:** total no of punctuation marks
*   **Upper Case Count:** total no of Upper Count words
*   **Title Word Count:** total no of proper case(title) words









"""

import string

x_train['char_count'] = x_train['Reviews'].apply(len)
x_train['word_count'] = x_train['Reviews'].apply(lambda x: len(x.split()))
x_train['Avg_word_density'] = x_train['char_count']/(x_train['word_count']+1)
# x_train['Punctutation_count'] = x_train['Reviews'].apply(lambda x: len("".join(i for i in x if i in string.punctuation)))
# x_train['Title_word_count'] = x_train['Reviews'].apply(lambda x: len([j for j in x.split() if j.istitle()]))

x_test['char_count'] = x_test['Reviews'].apply(len)
x_test['word_count'] = x_test['Reviews'].apply(lambda x: len(x.split()))
x_test['Avg_word_density'] = x_test['char_count']/(x_test['word_count']+1)
# x_test['Punctutation_count'] = x_test['Reviews'].apply(lambda x: len("".join(i for i in x if i in string.punctuation)))
# x_test['Title_word_count'] = x_test['Reviews'].apply(lambda x: len([j for j in x.split() if j.istitle()]))

x_train.info()

"""## **Trying to train a Logistic Regression model**"""

log_reg = LogisticRegression()
log_reg.fit(x_train.drop(['Reviews'], axis=1),y_train)

y_pred = log_reg.predict(x_test.drop(['Reviews'], axis=1))

print(pd.DataFrame(confusion_matrix(y_test, y_pred)))
print(classification_report(y_pred,y_test))

"""### The confusion matrix showss that our model was not able to predict a single 0 value, this was somehow expected as we have not used the sentiment analysis or any other way of reading the sentences so far

## **Using Text Sentiment**

TextBlob is an excellent open-source library for performing NLP tasks with ease, including sentiment analysis. It also an a sentiment lexicon (in the form of an XML file) which it leverages to give both polarity and subjectivity scores.


*   The polarity score is a float within the range [-1.0, 1.0].
*   The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.
"""

import textblob

x_train_snt_obj = x_train['Reviews'].apply(lambda row: textblob.TextBlob(row).sentiment)
x_train['Polarity'] = [obj.polarity for obj in x_train_snt_obj.values]
x_train['Subjectivity'] = [obj.subjectivity for obj in x_train_snt_obj.values]

x_test_snt_obj = x_test['Reviews'].apply(lambda row: textblob.TextBlob(row).sentiment)
x_test['Polarity'] = [obj.polarity for obj in x_test_snt_obj.values]
x_test['Subjectivity'] = [obj.subjectivity for obj in x_test_snt_obj.values]

x_train.head()

"""### **Training the Model and Evaluating its outcome**"""

log_reg.fit(x_train.drop(['Reviews'], axis=1),y_train)
pred = log_reg.predict(x_test.drop(['Reviews'], axis=1))
print(classification_report(y_test, pred))
cm = confusion_matrix(y_test, pred)
pd.DataFrame(cm)

"""### **Text Pre-Processing and Text Wrangling**
We will focus on:

*   Text Lowercasing
*   Removal of contractions
*   Removing unnecessary characters, numbers and symbols
*   Stemming
*   Stopword removal




"""

!pip install contractions
!pip install textsearch
!pip install tqdm
import nltk
nltk.download('punkt')
nltk.download('stopwords')

import contractions
import nltk
import re

contractions.fix('i didn\'t like this ping-pong bat')

# remove some stopwords to capture negation in n-grams if possible
stop_words = nltk.corpus.stopwords.words('english')
stop_words.remove('no')
stop_words.remove('not')
stop_words.remove('but')

# load up a simple porter stemmer - nothing fancy
ps = nltk.porter.PorterStemmer()

def simple_text_preprocessor(document): 
    # lower case
    document = str(document).lower()
    
    # expand contractions
    document = contractions.fix(document)
    
    # remove unnecessary characters
    document = re.sub(r'[^a-zA-Z]',r' ', document)
    document = re.sub(r'nbsp', r'', document)
    document = re.sub(' +', ' ', document)
    
    # simple porter stemming
    document = ' '.join([ps.stem(word) for word in document.split()])
    
    # stopwords removal
    document = ' '.join([word for word in document.split() if word not in stop_words])
    
    return document

stp = np.vectorize(simple_text_preprocessor)

x_train['Clean Reviews'] = stp(x_train['Reviews'].values)
x_test['Clean Reviews'] = stp(x_test['Reviews'].values)
x_train.head()

x_train_metadata = x_train.drop(['Reviews', 'Clean Reviews'], axis=1).reset_index(drop=True)
x_test_metadata = x_test.drop(['Reviews', 'Clean Reviews'], axis=1).reset_index(drop=True)

x_train_metadata.head()

"""## **Experiment 3:** 
###Adding Bag of Words based Features
"""

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(min_df = 0.0, max_df=1.0, ngram_range=(1,1))
x_train_cv = cv.fit_transform(x_train['Clean Reviews']).toarray()
x_train_cv = pd.DataFrame(x_train_cv, columns = cv.get_feature_names())

x_test_cv = cv.transform(x_test['Clean Reviews']).toarray()
x_test_cv = pd.DataFrame(x_test_cv, columns = cv.get_feature_names())

x_train_cv.head()

x_test_metadata

x_train_n = pd.concat([x_train_metadata, x_train_cv], axis=1)
x_test_n = pd.concat([x_test_metadata, x_test_cv], axis=1)

x_train_n.head()

"""### **Training the model with these new data and evaluating them**"""

log_reg.fit(x_train_n, y_train)
pred = log_reg.predict(x_test_n)

print(classification_report(y_test, pred))
pd.DataFrame(confusion_matrix(y_test, pred))

"""### **Our model is able to predict '0' with a precision of 77% and '1' with a precision of 90%.**
###**As well we are getting accuracy of almost 87%, which a very good accuracy for dealing with reviews.**

Now I am taking a random sentence as an example for a test Review to test wheather our pre trained model is able to predict the rating of it by analysing the statement only.

Firstly we will process the test data into such a format which our model will be able to read as i've already done.
Now we can test random sentences with below code.
"""

# pred_new = log_reg.predict('this product so amazing, the best one i bought')
test_1 = pd.DataFrame(['type here'])
test_1['char_count'] = test_1[0].apply(len)
test_1['word_count'] = test_1[0].apply(lambda x: len(x.split()))
test_1['Avg_word_density'] = test_1['char_count']/(x_test['word_count']+1)

test_1_snt_obj = test_1[0].apply(lambda row: textblob.TextBlob(row).sentiment)
test_1['Polarity'] = [obj.polarity for obj in test_1_snt_obj.values]
test_1['Subjectivity'] = [obj.subjectivity for obj in test_1_snt_obj.values]
# t = 'this product is so amazing, the best one i bought'
test_1

test_1_cv = cv.transform(test_1[0]).toarray()
test_1_cv = pd.DataFrame(test_1_cv, columns = cv.get_feature_names_out())
test_1_cv

test_2_cv = pd.concat([test_1, test_1_cv], axis=1)
test_2_cv
test_n = test_2_cv.iloc[:,1:]
test_n

pred_new = log_reg.predict(test_n)
pred_new

